{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils # Install poppler-utils using apt-get"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64asBxS-JuWp",
        "outputId": "96e102bb-7c56-4c89-e028-86d3dce5874d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 1s (210 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepface pytesseract opencv-python langdetect transformers googletrans PyPDF2 python-docx python-pptx pyspellchecker language-tool-python cryptography pdf2image retina-face"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XKDb7CbIXnx",
        "outputId": "8f7d97e4-4f25-4559-834f-bddc1226fb95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (43.0.3)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting retina-face\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.67.1)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (11.0.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.17.1)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (3.5.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (3.1.0)\n",
            "Collecting flask-cors>=4.0.1 (from deepface)\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Collecting httpx==0.13.3 (from googletrans)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.12.14)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans)\n",
            "  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (24.1.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (0.45.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography) (2.22)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.12.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2.2.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: langdetect, googletrans, fire\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=539ca4bf0b8b8a8c77d54d6336d22dacac160bc728f22df1688d03617d5bf3d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15719 sha256=e06a174d3a79d8ba2ce90e749a61371dd27727b9ddd23908361791bacea14b9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=0561856e4aa5835813fde07fca61cf82c235d11e5b63bb5c2b9919aec5758dd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built langdetect googletrans fire\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, XlsxWriter, python-docx, pytesseract, pyspellchecker, PyPDF2, pdf2image, lz4, langdetect, idna, hstspreload, h2, gunicorn, fire, python-pptx, mtcnn, httpcore, language-tool-python, httpx, flask-cors, googletrans, retina-face, deepface\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.2.3 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.57.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 XlsxWriter-3.2.0 chardet-3.0.4 deepface-0.0.93 fire-0.7.0 flask-cors-5.0.0 googletrans-3.0.0 gunicorn-23.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 langdetect-1.0.9 language-tool-python-2.8.1 lz4-4.3.3 mtcnn-1.0.0 pdf2image-1.17.0 pyspellchecker-0.8.2 pytesseract-0.3.13 python-docx-1.1.2 python-pptx-1.0.2 retina-face-0.0.17 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install retina-face"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBqDeAf-C9pv",
        "outputId": "43e04396-2ac8-4b03-e774-afa6dc335416"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: retina-face in /usr/local/lib/python3.10/dist-packages (0.0.17)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (1.26.4)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from retina-face) (5.2.0)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (11.0.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from retina-face) (4.10.0.84)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (2.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.68.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.12.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->retina-face) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (3.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->retina-face) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=1.9.0->retina-face) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_rxthUSuBPyw",
        "outputId": "f8e32ffc-8842-40f8-a7b8-c2d0d41a22bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.9\n",
            "  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (1.68.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (3.12.1)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (1.17.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9) (1.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (3.7)\n",
            "Collecting protobuf>=3.9.2 (from tensorflow==2.9)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.2.2)\n",
            "Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-aiplatform 1.74.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.16.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.7 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "f8d5a04595974676b8ca1c9048026f4b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCImX2CECoV1",
        "outputId": "74a08ca9-c9f4-4a3e-e6bd-c99a9fddcd71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,528 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123664 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DiDVzycQC0zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8f0ea1-2ab5-47db-9400-0ce0d0190ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose an option:\n",
            "1. Process a file\n",
            "2. Process real-time video\n",
            "Enter your choice: 1\n",
            "Enter the file path: a.pdf\n",
            "24-12-25 09:03:01 - retinaface.h5 will be downloaded from the url https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n",
            "To: /root/.deepface/weights/retinaface.h5\n",
            "100%|██████████| 119M/119M [00:00<00:00, 229MB/s] \n",
            "Action: age:   0%|          | 0/4 [00:00<?, ?it/s]    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24-12-25 09:03:18 - age_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/age_model_weights.h5\n",
            "To: /root/.deepface/weights/age_model_weights.h5\n",
            "\n",
            "  0%|          | 0.00/539M [00:00<?, ?B/s]\u001b[A\n",
            "  2%|▏         | 11.5M/539M [00:00<00:04, 114MB/s]\u001b[A\n",
            "  7%|▋         | 37.2M/539M [00:00<00:02, 197MB/s]\u001b[A\n",
            " 12%|█▏        | 63.4M/539M [00:00<00:02, 227MB/s]\u001b[A\n",
            " 16%|█▋        | 88.6M/539M [00:00<00:01, 234MB/s]\u001b[A\n",
            " 21%|██        | 112M/539M [00:00<00:01, 220MB/s] \u001b[A\n",
            " 25%|██▌       | 135M/539M [00:00<00:01, 211MB/s]\u001b[A\n",
            " 29%|██▉       | 158M/539M [00:00<00:01, 216MB/s]\u001b[A\n",
            " 34%|███▍      | 183M/539M [00:00<00:01, 227MB/s]\u001b[A\n",
            " 38%|███▊      | 206M/539M [00:00<00:01, 226MB/s]\u001b[A\n",
            " 43%|████▎     | 230M/539M [00:01<00:01, 228MB/s]\u001b[A\n",
            " 48%|████▊     | 256M/539M [00:01<00:01, 239MB/s]\u001b[A\n",
            " 52%|█████▏    | 280M/539M [00:01<00:01, 238MB/s]\u001b[A\n",
            " 57%|█████▋    | 305M/539M [00:01<00:00, 236MB/s]\u001b[A\n",
            " 61%|██████    | 329M/539M [00:01<00:00, 234MB/s]\u001b[A\n",
            " 65%|██████▌   | 352M/539M [00:01<00:00, 226MB/s]\u001b[A\n",
            " 70%|██████▉   | 375M/539M [00:01<00:00, 225MB/s]\u001b[A\n",
            " 74%|███████▍  | 398M/539M [00:01<00:00, 203MB/s]\u001b[A\n",
            " 78%|███████▊  | 419M/539M [00:01<00:00, 171MB/s]\u001b[A\n",
            " 81%|████████▏ | 438M/539M [00:02<00:00, 157MB/s]\u001b[A\n",
            " 84%|████████▍ | 455M/539M [00:02<00:00, 159MB/s]\u001b[A\n",
            " 87%|████████▋ | 471M/539M [00:02<00:00, 160MB/s]\u001b[A\n",
            " 91%|█████████ | 488M/539M [00:02<00:00, 136MB/s]\u001b[A\n",
            " 93%|█████████▎| 503M/539M [00:06<00:02, 13.6MB/s]\u001b[A\n",
            "100%|██████████| 539M/539M [00:06<00:00, 81.7MB/s]\n",
            "Action: gender:  25%|██▌       | 1/4 [00:13<00:40, 13.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24-12-25 09:03:33 - gender_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/gender_model_weights.h5\n",
            "To: /root/.deepface/weights/gender_model_weights.h5\n",
            "\n",
            "  0%|          | 0.00/537M [00:00<?, ?B/s]\u001b[A\n",
            "  1%|▏         | 7.86M/537M [00:00<00:06, 78.0MB/s]\u001b[A\n",
            "  6%|▌         | 33.0M/537M [00:00<00:02, 179MB/s] \u001b[A\n",
            " 10%|▉         | 51.4M/537M [00:00<00:03, 142MB/s]\u001b[A\n",
            " 12%|█▏        | 66.6M/537M [00:00<00:04, 113MB/s]\u001b[A\n",
            " 15%|█▍        | 79.2M/537M [00:01<00:07, 59.1MB/s]\u001b[A\n",
            " 16%|█▋        | 88.1M/537M [00:01<00:11, 37.9MB/s]\u001b[A\n",
            " 18%|█▊        | 94.9M/537M [00:01<00:13, 31.9MB/s]\u001b[A\n",
            " 19%|█▊        | 100M/537M [00:02<00:14, 29.2MB/s] \u001b[A\n",
            " 19%|█▉        | 104M/537M [00:02<00:15, 27.7MB/s]\u001b[A\n",
            " 20%|██        | 108M/537M [00:02<00:16, 26.5MB/s]\u001b[A\n",
            " 21%|██        | 111M/537M [00:02<00:16, 25.9MB/s]\u001b[A\n",
            " 21%|██▏       | 114M/537M [00:02<00:16, 25.5MB/s]\u001b[A\n",
            " 22%|██▏       | 117M/537M [00:02<00:16, 25.3MB/s]\u001b[A\n",
            " 22%|██▏       | 121M/537M [00:02<00:16, 25.2MB/s]\u001b[A\n",
            " 23%|██▎       | 123M/537M [00:03<00:16, 25.2MB/s]\u001b[A\n",
            " 23%|██▎       | 126M/537M [00:03<00:16, 25.4MB/s]\u001b[A\n",
            " 24%|██▍       | 129M/537M [00:03<00:15, 25.8MB/s]\u001b[A\n",
            " 25%|██▍       | 132M/537M [00:03<00:15, 26.2MB/s]\u001b[A\n",
            " 25%|██▌       | 135M/537M [00:03<00:15, 26.5MB/s]\u001b[A\n",
            " 26%|██▌       | 138M/537M [00:03<00:14, 26.8MB/s]\u001b[A\n",
            " 26%|██▋       | 142M/537M [00:03<00:14, 27.7MB/s]\u001b[A\n",
            " 27%|██▋       | 145M/537M [00:03<00:13, 28.1MB/s]\u001b[A\n",
            " 28%|██▊       | 148M/537M [00:03<00:13, 28.7MB/s]\u001b[A\n",
            " 28%|██▊       | 151M/537M [00:04<00:13, 29.0MB/s]\u001b[A\n",
            " 29%|██▊       | 154M/537M [00:04<00:13, 29.3MB/s]\u001b[A\n",
            " 29%|██▉       | 157M/537M [00:04<00:12, 29.8MB/s]\u001b[A\n",
            " 30%|██▉       | 160M/537M [00:04<00:12, 30.3MB/s]\u001b[A\n",
            " 31%|███       | 164M/537M [00:04<00:12, 30.8MB/s]\u001b[A\n",
            " 31%|███       | 167M/537M [00:04<00:12, 30.8MB/s]\u001b[A\n",
            " 32%|███▏      | 170M/537M [00:04<00:11, 30.8MB/s]\u001b[A\n",
            " 32%|███▏      | 174M/537M [00:04<00:11, 31.3MB/s]\u001b[A\n",
            " 33%|███▎      | 178M/537M [00:04<00:11, 32.1MB/s]\u001b[A\n",
            " 34%|███▍      | 181M/537M [00:05<00:10, 32.7MB/s]\u001b[A\n",
            " 34%|███▍      | 185M/537M [00:05<00:10, 33.1MB/s]\u001b[A\n",
            " 35%|███▌      | 189M/537M [00:05<00:10, 33.4MB/s]\u001b[A\n",
            " 36%|███▌      | 192M/537M [00:05<00:10, 33.8MB/s]\u001b[A\n",
            " 37%|███▋      | 197M/537M [00:05<00:09, 34.9MB/s]\u001b[A\n",
            " 37%|███▋      | 200M/537M [00:05<00:09, 35.2MB/s]\u001b[A\n",
            " 38%|███▊      | 204M/537M [00:05<00:09, 35.9MB/s]\u001b[A\n",
            " 39%|███▊      | 208M/537M [00:05<00:09, 35.9MB/s]\u001b[A\n",
            " 39%|███▉      | 212M/537M [00:05<00:09, 35.9MB/s]\u001b[A\n",
            " 40%|████      | 216M/537M [00:06<00:08, 36.5MB/s]\u001b[A\n",
            " 41%|████      | 220M/537M [00:06<00:08, 36.4MB/s]\u001b[A\n",
            " 42%|████▏     | 224M/537M [00:06<00:08, 37.3MB/s]\u001b[A\n",
            " 42%|████▏     | 228M/537M [00:06<00:08, 37.9MB/s]\u001b[A\n",
            " 43%|████▎     | 232M/537M [00:06<00:07, 38.8MB/s]\u001b[A\n",
            " 44%|████▍     | 236M/537M [00:06<00:07, 39.2MB/s]\u001b[A\n",
            " 45%|████▍     | 241M/537M [00:06<00:07, 39.9MB/s]\u001b[A\n",
            " 46%|████▌     | 245M/537M [00:06<00:07, 40.9MB/s]\u001b[A\n",
            " 46%|████▋     | 250M/537M [00:06<00:06, 41.1MB/s]\u001b[A\n",
            " 47%|████▋     | 254M/537M [00:06<00:06, 42.0MB/s]\u001b[A\n",
            " 48%|████▊     | 259M/537M [00:07<00:06, 41.9MB/s]\u001b[A\n",
            " 49%|████▉     | 263M/537M [00:07<00:06, 41.8MB/s]\u001b[A\n",
            " 50%|████▉     | 268M/537M [00:07<00:06, 42.5MB/s]\u001b[A\n",
            " 51%|█████     | 273M/537M [00:07<00:06, 42.7MB/s]\u001b[A\n",
            " 52%|█████▏    | 277M/537M [00:07<00:05, 43.7MB/s]\u001b[A\n",
            " 53%|█████▎    | 282M/537M [00:07<00:05, 43.8MB/s]\u001b[A\n",
            " 53%|█████▎    | 287M/537M [00:07<00:05, 43.1MB/s]\u001b[A\n",
            " 54%|█████▍    | 292M/537M [00:07<00:06, 39.1MB/s]\u001b[A\n",
            " 55%|█████▌    | 296M/537M [00:07<00:05, 40.5MB/s]\u001b[A\n",
            " 56%|█████▌    | 301M/537M [00:08<00:05, 42.7MB/s]\u001b[A\n",
            " 57%|█████▋    | 306M/537M [00:08<00:05, 43.7MB/s]\u001b[A\n",
            " 58%|█████▊    | 311M/537M [00:08<00:05, 44.4MB/s]\u001b[A\n",
            " 59%|█████▉    | 316M/537M [00:08<00:04, 45.7MB/s]\u001b[A\n",
            " 60%|█████▉    | 321M/537M [00:08<00:04, 46.5MB/s]\u001b[A\n",
            " 61%|██████    | 327M/537M [00:08<00:04, 47.0MB/s]\u001b[A\n",
            " 62%|██████▏   | 332M/537M [00:08<00:04, 47.6MB/s]\u001b[A\n",
            " 63%|██████▎   | 337M/537M [00:08<00:04, 48.7MB/s]\u001b[A\n",
            " 64%|██████▍   | 343M/537M [00:08<00:03, 49.9MB/s]\u001b[A\n",
            " 65%|██████▍   | 349M/537M [00:09<00:03, 50.6MB/s]\u001b[A\n",
            " 66%|██████▌   | 354M/537M [00:09<00:03, 51.9MB/s]\u001b[A\n",
            " 67%|██████▋   | 360M/537M [00:09<00:03, 51.4MB/s]\u001b[A\n",
            " 68%|██████▊   | 365M/537M [00:09<00:03, 51.4MB/s]\u001b[A\n",
            " 69%|██████▉   | 370M/537M [00:09<00:03, 51.5MB/s]\u001b[A\n",
            " 70%|██████▉   | 375M/537M [00:09<00:03, 51.7MB/s]\u001b[A\n",
            " 71%|███████   | 381M/537M [00:09<00:02, 52.5MB/s]\u001b[A\n",
            " 72%|███████▏  | 387M/537M [00:09<00:02, 52.3MB/s]\u001b[A\n",
            " 73%|███████▎  | 393M/537M [00:09<00:02, 54.2MB/s]\u001b[A\n",
            " 74%|███████▍  | 399M/537M [00:09<00:02, 55.0MB/s]\u001b[A\n",
            " 75%|███████▌  | 405M/537M [00:10<00:02, 55.4MB/s]\u001b[A\n",
            " 76%|███████▋  | 411M/537M [00:10<00:02, 55.0MB/s]\u001b[A\n",
            " 77%|███████▋  | 416M/537M [00:10<00:02, 54.0MB/s]\u001b[A\n",
            " 79%|███████▊  | 422M/537M [00:10<00:02, 54.3MB/s]\u001b[A\n",
            " 80%|███████▉  | 428M/537M [00:10<00:01, 55.1MB/s]\u001b[A\n",
            " 81%|████████  | 434M/537M [00:10<00:01, 55.5MB/s]\u001b[A\n",
            " 82%|████████▏ | 439M/537M [00:10<00:01, 56.1MB/s]\u001b[A\n",
            " 83%|████████▎ | 445M/537M [00:10<00:01, 56.1MB/s]\u001b[A\n",
            " 84%|████████▍ | 451M/537M [00:10<00:01, 56.1MB/s]\u001b[A\n",
            " 85%|████████▌ | 457M/537M [00:10<00:01, 57.6MB/s]\u001b[A\n",
            " 86%|████████▋ | 463M/537M [00:11<00:01, 58.9MB/s]\u001b[A\n",
            " 87%|████████▋ | 470M/537M [00:11<00:01, 59.9MB/s]\u001b[A\n",
            " 89%|████████▊ | 476M/537M [00:11<00:01, 60.5MB/s]\u001b[A\n",
            " 90%|████████▉ | 482M/537M [00:11<00:00, 60.6MB/s]\u001b[A\n",
            " 91%|█████████ | 489M/537M [00:11<00:00, 61.0MB/s]\u001b[A\n",
            " 92%|█████████▏| 495M/537M [00:11<00:00, 61.5MB/s]\u001b[A\n",
            " 93%|█████████▎| 501M/537M [00:11<00:00, 61.8MB/s]\u001b[A\n",
            " 95%|█████████▍| 508M/537M [00:11<00:00, 62.8MB/s]\u001b[A\n",
            " 96%|█████████▌| 514M/537M [00:11<00:00, 61.7MB/s]\u001b[A\n",
            " 97%|█████████▋| 521M/537M [00:12<00:00, 63.4MB/s]\u001b[A\n",
            " 98%|█████████▊| 528M/537M [00:12<00:00, 64.4MB/s]\u001b[A\n",
            "100%|██████████| 537M/537M [00:12<00:00, 43.7MB/s]\n",
            "Action: race:  50%|█████     | 2/4 [00:32<00:32, 16.49s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24-12-25 09:03:50 - race_model_single_batch.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/race_model_single_batch.h5\n",
            "To: /root/.deepface/weights/race_model_single_batch.h5\n",
            "\n",
            "  0%|          | 0.00/537M [00:00<?, ?B/s]\u001b[A\n",
            "  2%|▏         | 11.5M/537M [00:00<00:04, 114MB/s]\u001b[A\n",
            "  8%|▊         | 44.0M/537M [00:00<00:02, 236MB/s]\u001b[A\n",
            " 13%|█▎        | 71.8M/537M [00:00<00:01, 254MB/s]\u001b[A\n",
            " 18%|█▊        | 97.5M/537M [00:00<00:01, 238MB/s]\u001b[A\n",
            " 23%|██▎       | 122M/537M [00:00<00:02, 205MB/s] \u001b[A\n",
            " 28%|██▊       | 149M/537M [00:00<00:01, 226MB/s]\u001b[A\n",
            " 33%|███▎      | 178M/537M [00:00<00:01, 243MB/s]\u001b[A\n",
            " 38%|███▊      | 203M/537M [00:00<00:01, 241MB/s]\u001b[A\n",
            " 42%|████▏     | 228M/537M [00:01<00:01, 215MB/s]\u001b[A\n",
            " 47%|████▋     | 251M/537M [00:01<00:01, 179MB/s]\u001b[A\n",
            " 50%|█████     | 270M/537M [00:01<00:01, 158MB/s]\u001b[A\n",
            " 53%|█████▎    | 287M/537M [00:01<00:01, 160MB/s]\u001b[A\n",
            " 57%|█████▋    | 305M/537M [00:01<00:01, 164MB/s]\u001b[A\n",
            " 61%|██████    | 327M/537M [00:01<00:01, 176MB/s]\u001b[A\n",
            " 64%|██████▍   | 345M/537M [00:05<00:11, 16.6MB/s]\u001b[A\n",
            " 67%|██████▋   | 358M/537M [00:05<00:08, 20.5MB/s]\u001b[A\n",
            " 69%|██████▉   | 372M/537M [00:05<00:06, 25.9MB/s]\u001b[A\n",
            " 73%|███████▎  | 390M/537M [00:05<00:04, 35.4MB/s]\u001b[A\n",
            " 76%|███████▌  | 408M/537M [00:05<00:02, 47.9MB/s]\u001b[A\n",
            " 79%|███████▉  | 424M/537M [00:05<00:01, 58.8MB/s]\u001b[A\n",
            " 83%|████████▎ | 445M/537M [00:06<00:01, 78.2MB/s]\u001b[A\n",
            " 87%|████████▋ | 469M/537M [00:06<00:00, 103MB/s] \u001b[A\n",
            " 91%|█████████ | 489M/537M [00:06<00:00, 115MB/s]\u001b[A\n",
            " 94%|█████████▍| 507M/537M [00:06<00:00, 121MB/s]\u001b[A\n",
            "100%|██████████| 537M/537M [00:06<00:00, 82.2MB/s]\n",
            "Action: emotion:  75%|███████▌  | 3/4 [00:43<00:13, 13.97s/it]Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
            "To: /root/.deepface/weights/facial_expression_model_weights.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24-12-25 09:03:59 - facial_expression_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 5.98M/5.98M [00:00<00:00, 59.8MB/s]\n",
            "Action: emotion: 100%|██████████| 4/4 [00:43<00:00, 10.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File processed and saved.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import cv2\n",
        "import pytesseract\n",
        "from langdetect import detect\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from deepface import DeepFace\n",
        "from googletrans import Translator\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "from pptx import Presentation\n",
        "from spellchecker import SpellChecker\n",
        "from language_tool_python import LanguageTool\n",
        "from cryptography.fernet import Fernet\n",
        "from pdf2image import convert_from_path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.layers import Lambda\n",
        "from PIL import Image\n",
        "import os\n",
        "from retinaface import RetinaFace\n",
        "# Constants\n",
        "JSON_FILE = \"relations.json\"\n",
        "SECURE_DATA_FOLDER = \"secure_data\"\n",
        "ENCRYPTION_KEY_FILE = \"encryption_key.key\"\n",
        "LANGUAGE_MODEL = \"xlm-roberta-large-finetuned-conll03-english\"\n",
        "\n",
        "# Initialize Resources\n",
        "os.makedirs(SECURE_DATA_FOLDER, exist_ok=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(LANGUAGE_MODEL)\n",
        "model = AutoModelForTokenClassification.from_pretrained(LANGUAGE_MODEL)\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "translator = Translator()\n",
        "spell = SpellChecker()\n",
        "grammar_tool = LanguageTool('en')\n",
        "\n",
        "\n",
        "# Encryption Setup\n",
        "if not os.path.exists(ENCRYPTION_KEY_FILE):\n",
        "    key = Fernet.generate_key()\n",
        "    with open(ENCRYPTION_KEY_FILE, 'wb') as key_file:\n",
        "        key_file.write(key)\n",
        "else:\n",
        "    with open(ENCRYPTION_KEY_FILE, 'rb') as key_file:\n",
        "        key = key_file.read()\n",
        "\n",
        "cipher = Fernet(key)\n",
        "\n",
        "# Load or Initialize JSON\n",
        "def load_json(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'rb') as file:\n",
        "            encrypted_data = file.read()\n",
        "        # decrypted_data = cipher.decrypt(encrypted_data).decode('utf-8')\n",
        "        return json.loads(encrypted_data.decode('utf-8'))\n",
        "    return []\n",
        "\n",
        "def save_json(data, file_path):\n",
        "    json_data = json.dumps(data, indent=4)\n",
        "    # encrypted_data = cipher.encrypt(json_data.encode('utf-8'))\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(json_data.encode('utf-8'))\n",
        "\n",
        "# Text Cleaning: Correct Spelling, Grammar, and Abbreviations\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by correcting spelling, grammar, and abbreviations.\n",
        "    Also handles language detection errors by assuming English if detection fails.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    # Check if text is empty or contains only whitespace\n",
        "    if not text or text.isspace():\n",
        "        return text  # Return original text if empty or whitespace\n",
        "\n",
        "    try:\n",
        "        detected_lang = detect(text)\n",
        "    except LangDetectException:\n",
        "        detected_lang = 'en'  # Assume English if detection fails\n",
        "\n",
        "    if detected_lang != 'en':\n",
        "        text = translator.translate(text, src=detected_lang, dest='en').text\n",
        "\n",
        "    # Spell correction\n",
        "    words = text.split()\n",
        "    corrected_words = [spell.correction(word) for word in words]\n",
        "    text = \" \".join(corrected_words)\n",
        "\n",
        "    # Grammar correction\n",
        "    text = grammar_tool.correct(text)\n",
        "\n",
        "    # Replace abbreviations\n",
        "    abbreviations = {\n",
        "        \"DOB\": \"Date of Birth\",\n",
        "        \"ID\": \"Identification\",\n",
        "        \"No.\": \"Number\",\n",
        "    }\n",
        "    for abbr, full_form in abbreviations.items():\n",
        "        text = text.replace(abbr, full_form)\n",
        "    return text\n",
        "\n",
        "# Extract Entities and Relationships\n",
        "def extract_entities_and_relationships(text):\n",
        "    text = clean_text(text)\n",
        "    entities = ner_pipeline(text)\n",
        "    return {\"entities\": entities, \"relationships\": []}  # Placeholder for relationships\n",
        "\n",
        "# Extract Images from Documents\n",
        "def extract_images_from_pdf(file_path, output_folder):\n",
        "    images = convert_from_path(file_path)\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        output_path = os.path.join(output_folder, f\"pdf_image_{i + 1}.jpg\")\n",
        "        image.save(output_path, \"JPEG\")\n",
        "        image_paths.append(output_path)\n",
        "    return image_paths\n",
        "\n",
        "def extract_images_from_docx(file_path, output_folder):\n",
        "    doc = Document(file_path)\n",
        "    image_paths = []\n",
        "    for i, rel in enumerate(doc.part.rels.values()):\n",
        "        if \"image\" in rel.target_ref:\n",
        "            image_data = rel.target_part.blob\n",
        "            output_path = os.path.join(output_folder, f\"docx_image_{i + 1}.jpg\")\n",
        "            with open(output_path, \"wb\") as img_file:\n",
        "                img_file.write(image_data)\n",
        "            image_paths.append(output_path)\n",
        "    return image_paths\n",
        "\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "def extract_images_from_pdf(file_path, output_folder):\n",
        "  # Set poppler_path to your installation directory here\n",
        "  # The default for most systems is '/usr/bin'\n",
        "  poppler_path = '/usr/bin'\n",
        "  images = convert_from_path(file_path, poppler_path=poppler_path)\n",
        "  image_paths = []\n",
        "  for i, image in enumerate(images):\n",
        "      output_path = os.path.join(output_folder, f\"pdf_image_{i + 1}.jpg\")\n",
        "      image.save(output_path, \"JPEG\")\n",
        "      image_paths.append(output_path)\n",
        "  return image_paths\n",
        "# File Type Handlers\n",
        "def extract_text_from_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    return \" \".join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    return \" \".join(paragraph.text for paragraph in doc.paragraphs)\n",
        "\n",
        "def extract_text_from_pptx(file_path):\n",
        "    ppt = Presentation(file_path)\n",
        "    return \" \".join(shape.text for slide in ppt.slides for shape in slide.shapes if hasattr(shape, \"text\"))\n",
        "\n",
        "def extract_text_from_excel(file_path):\n",
        "    data = pd.read_excel(file_path)\n",
        "    return data.to_string(index=False)\n",
        "\n",
        "def analyze_faces_in_images(image_paths):\n",
        "    # Load the face cascade classifier\n",
        "    # face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    faces_info = []\n",
        "    output_dir = \"extracted_faces\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for image_path in image_paths:\n",
        "            faces = RetinaFace.extract_faces(img_path=image_path, align=True)\n",
        "            # Save each face as a separate image\n",
        "            for i, face in enumerate(faces):\n",
        "                # Convert the NumPy array to a PIL Image\n",
        "                face_image = Image.fromarray(face)\n",
        "                path=os.path.join(output_dir, f\"face_{i + 1}.jpg\")\n",
        "                # Save the image with a unique name\n",
        "                face_image.save(path)\n",
        "                result = DeepFace.analyze(img_path = path, actions = ['age', 'gender', 'race', 'emotion'],)\n",
        "                faces_info.append(result)\n",
        "    return faces_info\n",
        "\n",
        "# File Type Handlers\n",
        "def process_file(file_path):\n",
        "    _, ext = os.path.splitext(file_path)\n",
        "    ext = ext.lower()\n",
        "    output_folder = SECURE_DATA_FOLDER\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "        images = extract_images_from_pdf(file_path, output_folder)\n",
        "        if len(images) != 0:\n",
        "          for path in images:\n",
        "              text.join(pytesseract.image_to_string(path, lang=\"eng\"))\n",
        "    elif ext == \".docx\":\n",
        "        text = extract_text_from_docx(file_path)\n",
        "        images = extract_images_from_docx(file_path, output_folder)\n",
        "        if len(images) != 0:\n",
        "          for path in images:\n",
        "              text.join(pytesseract.image_to_string(path, lang=\"eng\"))\n",
        "    elif ext == \".pptx\":\n",
        "        text = extract_text_from_pptx(file_path)\n",
        "        images = []\n",
        "    elif ext in [\".xls\", \".xlsx\"]:\n",
        "        text = extract_text_from_excel(file_path)\n",
        "        images = []\n",
        "    elif ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "        text = pytesseract.image_to_string(file_path, lang=\"eng\")\n",
        "        images = [file_path]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
        "\n",
        "    # Analyze Faces from images\n",
        "    faces_info = analyze_faces_in_images(images)\n",
        "    entities = extract_entities_and_relationships(text)\n",
        "    data = {\n",
        "        \"file_type\": ext.strip(\".\"),\n",
        "        \"text_content\": text,\n",
        "        \"images\": images,\n",
        "        \"faces_info\": faces_info,\n",
        "        \"entities\": entities[\"entities\"]\n",
        "    }\n",
        "    return data\n",
        "\n",
        "# Main Workflow\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Choose an option:\")\n",
        "    print(\"1. Process a file\")\n",
        "    print(\"2. Process real-time video\")\n",
        "    choice = int(input(\"Enter your choice: \"))\n",
        "\n",
        "    if choice == 1:\n",
        "        file_path = input(\"Enter the file path: \")\n",
        "        try:\n",
        "            processed_data = process_file(file_path)\n",
        "            relations = load_json(JSON_FILE)\n",
        "            relations.append(processed_data)\n",
        "            save_json(relations, JSON_FILE)\n",
        "            print(\"File processed and saved.\")\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "    elif choice == 2:\n",
        "        print(\"Real-time video processing is not yet encrypted.\")\n",
        "    else:\n",
        "        print(\"Invalid choice.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import cv2\n",
        "import pytesseract\n",
        "from langdetect import detect\n",
        "# from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from deepface import DeepFace\n",
        "from googletrans import Translator\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "from pptx import Presentation\n",
        "from spellchecker import SpellChecker\n",
        "from language_tool_python import LanguageTool\n",
        "from cryptography.fernet import Fernet\n",
        "from pdf2image import convert_from_path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.layers import Lambda\n",
        "import os\n",
        "# Constants\n",
        "JSON_FILE = \"relations.json\"\n",
        "SECURE_DATA_FOLDER = \"secure_data\"\n",
        "ENCRYPTION_KEY_FILE = \"encryption_key.key\"\n",
        "LANGUAGE_MODEL = \"xlm-roberta-large-finetuned-conll03-english\"\n",
        "pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "# Initialize Resources\n",
        "os.makedirs(SECURE_DATA_FOLDER, exist_ok=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(LANGUAGE_MODEL)\n",
        "model = AutoModelForTokenClassification.from_pretrained(LANGUAGE_MODEL)\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "translator = Translator()\n",
        "spell = SpellChecker()\n",
        "grammar_tool = LanguageTool('en')\n",
        "\n",
        "# Encryption Setup\n",
        "if not os.path.exists(ENCRYPTION_KEY_FILE):\n",
        "    key = Fernet.generate_key()\n",
        "    with open(ENCRYPTION_KEY_FILE, 'wb') as key_file:\n",
        "        key_file.write(key)\n",
        "else:\n",
        "    with open(ENCRYPTION_KEY_FILE, 'rb') as key_file:\n",
        "        key = key_file.read()\n",
        "\n",
        "cipher = Fernet(key)\n",
        "\n",
        "# Load or Initialize JSON\n",
        "def load_json(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'rb') as file:\n",
        "            encrypted_data = file.read()\n",
        "        # decrypted_data = cipher.decrypt(encrypted_data).decode('utf-8')\n",
        "        return json.loads(encrypted_data.decode('utf-8'))\n",
        "    return []\n",
        "\n",
        "def save_json(data, file_path):\n",
        "    json_data = json.dumps(data, indent=4)\n",
        "    # encrypted_data = cipher.encrypt(json_data.encode('utf-8'))\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(json_data.encode('utf-8'))\n",
        "\n",
        "# Text Cleaning: Correct Spelling, Grammar, and Abbreviations\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by correcting spelling, grammar, and abbreviations.\n",
        "    Also handles language detection errors by assuming English if detection fails.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    # Check if text is empty or contains only whitespace\n",
        "    if not text or text.isspace():\n",
        "        return text  # Return original text if empty or whitespace\n",
        "\n",
        "    try:\n",
        "        detected_lang = detect(text)\n",
        "    except LangDetectException:\n",
        "        detected_lang = 'en'  # Assume English if detection fails\n",
        "\n",
        "    if detected_lang != 'en':\n",
        "        text = translator.translate(text, src=detected_lang, dest='en').text\n",
        "\n",
        "    # Spell correction\n",
        "    words = text.split()\n",
        "    corrected_words = [str(spell.correction(word)) for word in words]\n",
        "    print(corrected_words)\n",
        "    text = \" \".join(corrected_words)\n",
        "\n",
        "    # Grammar correction\n",
        "    text = grammar_tool.correct(text)\n",
        "\n",
        "    # Replace abbreviations\n",
        "    abbreviations = {\n",
        "        \"DOB\": \"Date of Birth\",\n",
        "        \"ID\": \"Identification\",\n",
        "        \"No.\": \"Number\",\n",
        "    }\n",
        "    for abbr, full_form in abbreviations.items():\n",
        "        text = text.replace(abbr, full_form)\n",
        "    return text\n",
        "\n",
        "# Extract Entities and Relationships\n",
        "def extract_entities_and_relationships(text):\n",
        "    text = clean_text(text)\n",
        "    entities = ner_pipeline(text)\n",
        "    return {\"entities\": entities, \"relationships\": []}  # Placeholder for relationships\n",
        "\n",
        "# Extract Images from Documents\n",
        "def extract_images_from_pdf(file_path, output_folder):\n",
        "    images = convert_from_path(file_path)\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        output_path = os.path.join(output_folder, f\"pdf_image_{i + 1}.jpg\")\n",
        "        image.save(output_path, \"JPEG\")\n",
        "        image_paths.append(output_path)\n",
        "    return image_paths\n",
        "\n",
        "def extract_images_from_docx(file_path, output_folder):\n",
        "    doc = Document(file_path)\n",
        "    image_paths = []\n",
        "    for i, rel in enumerate(doc.part.rels.values()):\n",
        "        if \"image\" in rel.target_ref:\n",
        "            image_data = rel.target_part.blob\n",
        "            output_path = os.path.join(output_folder, f\"docx_image_{i + 1}.jpg\")\n",
        "            with open(output_path, \"wb\") as img_file:\n",
        "                img_file.write(image_data)\n",
        "            image_paths.append(output_path)\n",
        "    return image_paths\n",
        "\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "def extract_images_from_pdf(file_path, output_folder):\n",
        "  # Set poppler_path to your installation directory here\n",
        "  # The default for most systems is '/usr/bin'\n",
        "  poppler_path = '/usr/bin'\n",
        "  images = convert_from_path(file_path, poppler_path=poppler_path)\n",
        "  image_paths = []\n",
        "  for i, image in enumerate(images):\n",
        "      output_path = os.path.join(output_folder, f\"pdf_image_{i + 1}.jpg\")\n",
        "      image.save(output_path, \"JPEG\")\n",
        "      image_paths.append(output_path)\n",
        "  return image_paths\n",
        "# File Type Handlers\n",
        "def extract_text_from_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    return \" \".join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    return \" \".join(paragraph.text for paragraph in doc.paragraphs)\n",
        "\n",
        "def extract_text_from_pptx(file_path):\n",
        "    ppt = Presentation(file_path)\n",
        "    return \" \".join(shape.text for slide in ppt.slides for shape in slide.shapes if hasattr(shape, \"text\"))\n",
        "\n",
        "def extract_text_from_excel(file_path):\n",
        "    data = pd.read_excel(file_path)\n",
        "    return data.to_string(index=False)\n",
        "\n",
        "def analyze_faces_in_images(image_paths):\n",
        "    # Load the face cascade classifier\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    faces_info = []\n",
        "    for image_path in image_paths:\n",
        "        # try:\n",
        "            # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Could not read the image {image_path}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Convert to grayscale for face detection\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            for (x, y, w, h) in faces:\n",
        "                # Crop and prepare the face image\n",
        "                face_image = image[y:y + h, x:x + w]\n",
        "                face_image_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Save cropped face temporarily\n",
        "                temp_path = \"temp_face_image.jpg\"\n",
        "                cv2.imwrite(temp_path, face_image_rgb)\n",
        "\n",
        "                # Perform analysis using DeepFace\n",
        "                result = DeepFace.analyze(temp_path, actions=[\"age\", \"gender\", \"emotion\"], detector_backend=\"retinaface\")\n",
        "                faces_info.append(result)\n",
        "\n",
        "                # Remove the temporary file\n",
        "                # os.remove(temp_path)\n",
        "        else:\n",
        "            print(f\"No faces detected in {image_path}\")\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error analyzing image {image_path}: {e}\")\n",
        "\n",
        "    return faces_info\n",
        "\n",
        "# File Type Handlers\n",
        "def process_file(file_path):\n",
        "    _, ext = os.path.splitext(file_path)\n",
        "    ext = ext.lower()\n",
        "    output_folder = SECURE_DATA_FOLDER\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "        images = extract_images_from_pdf(file_path, output_folder)\n",
        "        if len(images) != 0:\n",
        "          for path in images:\n",
        "              text.join(pytesseract.image_to_string(path, lang=\"eng\"))\n",
        "    elif ext == \".docx\":\n",
        "        text = extract_text_from_docx(file_path)\n",
        "        images = extract_images_from_docx(file_path, output_folder)\n",
        "        if len(images) != 0:\n",
        "          for path in images:\n",
        "              text.join(pytesseract.image_to_string(path, lang=\"eng\"))\n",
        "    elif ext == \".pptx\":\n",
        "        text = extract_text_from_pptx(file_path)\n",
        "        images = []\n",
        "    elif ext in [\".xls\", \".xlsx\"]:\n",
        "        text = extract_text_from_excel(file_path)\n",
        "        images = []\n",
        "    elif ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "        text = pytesseract.image_to_string(file_path, lang=\"eng\")\n",
        "        images = [file_path]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
        "\n",
        "    # Analyze Faces from images\n",
        "    faces_info = analyze_faces_in_images(images)\n",
        "\n",
        "    entities = extract_entities_and_relationships(text)\n",
        "    data = {\n",
        "        \"file_type\": ext.strip(\".\"),\n",
        "        \"text_content\": text,\n",
        "        \"images\": images,\n",
        "        \"faces_info\": faces_info,\n",
        "        \"entities\": entities[\"entities\"]\n",
        "    }\n",
        "    return data\n",
        "\n",
        "# Main Workflow\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Choose an option:\")\n",
        "    print(\"1. Process a file\")\n",
        "    print(\"2. Process real-time video\")\n",
        "    choice = int(input(\"Enter your choice: \"))\n",
        "\n",
        "    if choice == 1:\n",
        "        file_path = input(\"Enter the file path: \")\n",
        "        try:\n",
        "            processed_data = process_file(file_path)\n",
        "            relations = load_json(JSON_FILE)\n",
        "            relations.append(processed_data)\n",
        "            save_json(str(relations), JSON_FILE)\n",
        "            print(\"File processed and saved.\")\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "    elif choice == 2:\n",
        "        print(\"Real-time video processing is not yet encrypted.\")\n",
        "    else:\n",
        "        print(\"Invalid choice.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVa0u89WL9o7",
        "outputId": "ad41da27-5a49-4a24-e957-2f23dd8f0294"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose an option:\n",
            "1. Process a file\n",
            "2. Process real-time video\n",
            "Enter your choice: 1\n",
            "Enter the file path: a.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Action: emotion: 100%|██████████| 3/3 [00:01<00:00,  2.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Face could not be detected in temp_face_image.jpg.Please confirm that the picture is a face photo or consider to set enforce_detection param to False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tesseract-ocr -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2FNk5XZNB8A",
        "outputId": "abc19a15-8ceb-4ab6-c10b-fdb98b7b1b60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (8,472 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123664 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    }
  ]
}